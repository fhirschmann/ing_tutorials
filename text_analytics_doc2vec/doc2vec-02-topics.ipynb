{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a doc2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do in this exercise:\n",
    "* load a pre-trained doc2vec model\n",
    "* use it to infer document embeddings for our test set\n",
    "* cluster the documents based on the embeddings cosine distances\n",
    "* use t-SNE to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import doc2vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.cluster import kmeans\n",
    "from nltk.cluster import util\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic settings\n",
    "HOMEDIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FILE = os.path.join(HOMEDIR, \"data/train_docs.txt\")\n",
    "MODEL_FILE_DM = os.path.join(HOMEDIR, \"models/doc2vec_DM_v20171229.bin\")\n",
    "MODEL_FILE_DBOW = os.path.join(HOMEDIR, \"models/doc2vec_DBOW_v20171229.bin\")\n",
    "\n",
    "NUM_CLUSTERS = 20  # yes, you can change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read corpus file and parse into token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    docs = [simple_preprocess(line, deacc=False, min_len=1) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read existing model and use it to derive document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = doc2vec.Doc2Vec.load(MODEL_FILE_DM)  # DM model chosen by default\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)  # only keep what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "**_Exercise 2: Combine DM and DBOW models_**\n",
    "\n",
    "**Note: don't start this exercise yet! First complete the rest of the notebook, then return here to do this exercise!**\n",
    "\n",
    "The authors of the paper suggest that combining the DM and the DBOW model works better than any single one. Do this by concatenating (you could also try to averaging or summing) the embeddings from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "del model\n",
    "model_dm = doc2vec.Doc2Vec.load(MODEL_FILE_DM)\n",
    "model_dbow = doc2vec.Doc2Vec.load(MODEL_FILE_DBOW)\n",
    "\n",
    "docvecs_dm = [model_dm.infer_vector(d, alpha=0.01, steps=1000) for d in docs]\n",
    "docvecs_dbow = [model_dbow.infer_vector(d, alpha=0.01, steps=1000) for d in docs]\n",
    "\n",
    "docvecs = [docvecs_dm[i] + docvecs_dbow[i] for i, d in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown"
   },
   "source": [
    "=========== end of exercise ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer document vectors\n",
    "docvecs = [model.infer_vector(d, alpha=0.01, steps=1000) for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have document vectors, start clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = kmeans.KMeansClusterer(NUM_CLUSTERS, distance=util.cosine_distance, repeats=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = clusterer.cluster(docvecs, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 27,\n",
       "         1: 36,\n",
       "         2: 20,\n",
       "         3: 100,\n",
       "         4: 15,\n",
       "         5: 146,\n",
       "         6: 45,\n",
       "         7: 39,\n",
       "         8: 69,\n",
       "         9: 6,\n",
       "         10: 75,\n",
       "         11: 30,\n",
       "         12: 86,\n",
       "         13: 38,\n",
       "         14: 72,\n",
       "         15: 47,\n",
       "         16: 37,\n",
       "         17: 47,\n",
       "         18: 28,\n",
       "         19: 37})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many documents per cluster?\n",
    "collections.Counter(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_in_cluster(cluster_idx):\n",
    "    return [doc for i, doc in enumerate(docs) if cluster_assignments[i] == cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topics(doc_vec, topic_vecs):\n",
    "    \"\"\"\n",
    "    For a given document, give the topic distribution (softmax probabilities for all topics)\n",
    "    \"\"\"\n",
    "    similarities = [np.dot(doc_vec, topic_vec) for topic_vec in topic_vecs]\n",
    "    return np.exp(similarities) / np.sum(np.exp(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define the topics as the cluster centroids. Then find the nearest-neighbor words to describe the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vecs = clusterer.means()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize topics using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do now:\n",
    "* reduce 100-dim vector space to 2 dimensions\n",
    "* plot all documents in this 2D space\n",
    "* use color to show the clustering\n",
    "* inspect how close / afar certain documents are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import push_notebook, output_notebook, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tsne = TSNE(n_components=2, perplexity=30, init='pca').fit_transform(docvecs)\n",
    "docs_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix with topic proportion per doc per topic\n",
    "doc_topic_matrix = [get_document_topics(docvec, topic_vecs) for docvec in docvecs]\n",
    "# select highest topic prob\n",
    "prob_max_topic = np.max(doc_topic_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedata = {\n",
    "    'x': docs_tsne[:, 0],\n",
    "    'y': docs_tsne[:, 1],\n",
    "    'color': colormap[cluster_assignments],\n",
    "    'alpha': prob_max_topic * 50,\n",
    "    'content': lines,\n",
    "    'topic_key': cluster_assignments\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot = bp.figure(plot_width=1600, plot_height=900,\n",
    "                      title=\"Topics\",\n",
    "                      tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                      x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "tsne_plot.scatter(x='x', \n",
    "                  y='y',\n",
    "                  color='color',\n",
    "                  size='alpha',\n",
    "                  #size=10,\n",
    "                  source=bp.ColumnDataSource(sourcedata)\n",
    "                 )\n",
    "\n",
    "# add hover tooltips\n",
    "hover = tsne_plot.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok. Now go back up and start exercise 2 and see if it's an improvement!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
