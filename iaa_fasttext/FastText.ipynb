{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText - Word Vectors & Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to the IAA FastText tutorial!\n",
    "\n",
    "In this tutorial, we will dive into the following topics:\n",
    "\n",
    "- Word vectors and continuous word space visualization\n",
    "- Text classification\n",
    "\n",
    "You will be able to experiment freely with word vectors and then move onto classifying text into categories. So, let's get started!\n",
    "\n",
    "We'll be using FastText from Facebook throughout this tutorial. FastText is a highly optimized open-source tool that serves the following three purposes:\n",
    "\n",
    "- Learning vector representations for words. See [this paper](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "- Classifying text into categories. See [this paper](https://arxiv.org/pdf/1607.01759.pdf).\n",
    "- Compressing these models to work on mobile devices. See [this paper](https://arxiv.org/pdf/1612.03651.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representations via Skipgram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, the research group at Google around Thomas Mikolov [introduced](https://arxiv.org/pdf/1301.3781.pdf) two models for learning vector representations for words from very large data sets. In this tutorial, we will concentrate on the Skipgram model.\n",
    "\n",
    "The skipgram model is surprisingly simple: Given a sentence, it predicts the surrounding words given the current word. Due to limited time in this tutorial, we cannot explain this model in detail. Please refer to [the original paper](https://arxiv.org/pdf/1301.3781.pdf), [our presentation in the team meeting](https://orangesharing.com/confluence/download/attachments/35651640/DOCLA%20FINAL_v3.pptx?version=1&modificationDate=1510305214047&api=v2) as well as [this excellent blog post](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).\n",
    "\n",
    "Facebook provides pre-trained models trained on Wikipedia. We have prepared these models in this workspace. So, let's load them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_model = fastText.load_model(\"wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded the pre-trained skipgram model. Note that this model was trained using sub-word information, meaning that a word is composed of a set of n-grams. We can see the n-grams a made is up of using the `get_subwords` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<Wi',\n",
       "  '<Wik',\n",
       "  '<Wiki',\n",
       "  '<Wikip',\n",
       "  'Wik',\n",
       "  'Wiki',\n",
       "  'Wikip',\n",
       "  'Wikipe',\n",
       "  'iki',\n",
       "  'ikip',\n",
       "  'ikipe',\n",
       "  'ikiped',\n",
       "  'kip',\n",
       "  'kipe',\n",
       "  'kiped',\n",
       "  'kipedi',\n",
       "  'ipe',\n",
       "  'iped',\n",
       "  'ipedi',\n",
       "  'ipedia',\n",
       "  'ped',\n",
       "  'pedi',\n",
       "  'pedia',\n",
       "  'pedia>',\n",
       "  'edi',\n",
       "  'edia',\n",
       "  'edia>',\n",
       "  'dia',\n",
       "  'dia>',\n",
       "  'ia>'],\n",
       " array([2687521, 4176702, 4464113, 3365183, 2891004, 3368715, 2528365,\n",
       "        3703676, 3360838, 3088430, 3734365, 2960415, 3583541, 4103636,\n",
       "        3494260, 2885315, 3309957, 3864535, 3689398, 3311169, 3519608,\n",
       "        3537807, 3432822, 4513568, 3119881, 3214276, 3085910, 3969639,\n",
       "        2531043, 2799581]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_model.get_subwords(\"Wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous output, the `<` and `>` indicate the beginning and end of a word.\n",
    "\n",
    "The vector representation of a word can be queries using the `get_word_vector` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19710374, -0.32406542, -0.27992466,  0.03215932, -0.26347142,\n",
       "       -0.17406814,  0.27865204, -0.35268489, -0.06575041,  0.16746461,\n",
       "       -0.22918601,  0.14560759, -0.08067662,  0.15618187,  0.05974622,\n",
       "       -0.19944344, -0.14420216,  0.05859416, -0.14302237,  0.19625185,\n",
       "       -0.17889747,  0.42659137, -0.21454638,  0.01065555, -0.0725894 ,\n",
       "       -0.06344882, -0.12478225,  0.21429643, -0.26189515,  0.12573811,\n",
       "       -0.3438704 ,  0.13429029, -0.20808646,  0.02837938, -0.37477136,\n",
       "       -0.27997339, -0.19806536,  0.07779928, -0.07717752, -0.0728948 ,\n",
       "       -0.09503662,  0.00841476, -0.03266108,  0.1387451 , -0.01365832,\n",
       "        0.47560605, -0.17646891, -0.16366199,  0.01501894,  0.10416513,\n",
       "        0.04873297, -0.34947178,  0.03553875, -0.045429  , -0.15141396,\n",
       "        0.29017088,  0.04766348, -0.38104695,  0.15794057,  0.35131243,\n",
       "        0.06430718,  0.12601593,  0.31181747, -0.1092104 ,  0.03415869,\n",
       "       -0.23455599,  0.51612747,  0.27065933, -0.16340432, -0.00415592,\n",
       "        0.41387683,  0.0868362 ,  0.08418673,  0.1281354 , -0.33774686,\n",
       "       -0.0381    ,  0.19683212,  0.41847378, -0.30517182, -0.37289214,\n",
       "        0.03312356,  0.54918611, -0.03005597,  0.12246966,  0.19055177,\n",
       "        0.11347365, -0.26005876,  0.01565691, -0.03605472,  0.22462089,\n",
       "        0.28091076,  0.19373532,  0.03321373, -0.30010328,  0.19812235,\n",
       "        0.26374829, -0.46301952, -0.18789612,  0.22373322,  0.04846102,\n",
       "        0.11103263,  0.09155527,  0.2053906 ,  0.01278809,  0.17770608,\n",
       "        0.17860056, -0.2853756 ,  0.72325355,  0.42728728,  0.43419361,\n",
       "       -0.36598557, -0.01752059,  0.10046127, -0.29695961, -0.15861763,\n",
       "       -0.20518515, -0.05701745,  0.07565837,  0.01055726,  0.17692082,\n",
       "        0.30536896,  0.0047566 , -0.02101283,  0.2942166 ,  0.27309597,\n",
       "        0.03214707, -0.03389257,  0.02551763, -0.03608243,  0.36184353,\n",
       "        0.0745113 , -0.61380148, -0.24259521,  0.04332252,  0.16967115,\n",
       "        0.11000641, -0.3639065 , -0.26339278,  0.47042191,  0.22318839,\n",
       "       -0.23435098,  0.27582222,  0.15671371, -0.0210599 , -0.00112069,\n",
       "       -0.43076876,  0.21643236,  0.12935966,  0.17283484, -0.21200299,\n",
       "        0.33966291, -0.03740753,  0.07441072, -0.18481664,  0.00554946,\n",
       "       -0.29739565,  0.08112556, -0.01376444, -0.05030123,  0.05489472,\n",
       "       -0.10589398,  0.03886421, -0.36626029, -0.30888349,  0.08217132,\n",
       "       -0.20062177, -0.08441073, -0.43861264, -0.21326244,  0.13926767,\n",
       "       -0.08157486, -0.13900165, -0.49082792, -0.24656297, -0.28904459,\n",
       "        0.01579446, -0.07375654, -0.06757743,  0.16351022, -0.30362961,\n",
       "       -0.28087255,  0.20484655, -0.32714769,  0.08165012, -0.10642418,\n",
       "        0.13275188,  0.06230205, -0.34879822,  0.12365136, -0.07247093,\n",
       "       -0.03676243, -0.29346859,  0.15082376,  0.12244092,  0.17061326,\n",
       "       -0.13683221, -0.20729709,  0.21168201, -0.08442178, -0.07648416,\n",
       "        0.25205025, -0.22266756, -0.29133347, -0.07587958,  0.16013953,\n",
       "        0.32215726, -0.35292971, -0.01745701,  0.11313796, -0.10227087,\n",
       "        0.08873294,  0.01168167,  0.08652534, -0.23880728, -0.06769008,\n",
       "       -0.11319653, -0.16597399,  0.04917406, -0.03970169,  0.35426468,\n",
       "        0.1230854 ,  0.15667512, -0.10348479, -0.18610653,  0.0487701 ,\n",
       "        0.49998692,  0.09971341, -0.1313937 , -0.01593362,  0.2096255 ,\n",
       "        0.25328463,  0.19817789, -0.22936803,  0.06175553,  0.29649603,\n",
       "       -0.24513485,  0.17497303, -0.04027266,  0.1989916 , -0.34992182,\n",
       "        0.08523175,  0.11396902,  0.22283871,  0.0869915 , -0.16560765,\n",
       "        0.08729383,  0.28117487,  0.2932885 ,  0.0565325 , -0.15089132,\n",
       "       -0.41896114,  0.11403888, -0.31483108,  0.27622569,  0.13715747,\n",
       "       -0.14149575, -0.05427691, -0.25447235,  0.14242369,  0.02079599,\n",
       "        0.21597184,  0.00383267, -0.02500198,  0.08601119, -0.1725405 ,\n",
       "        0.03845881, -0.02413942, -0.08684855, -0.38474005,  0.06693038,\n",
       "        0.09952013,  0.07195403,  0.6351825 , -0.03426036, -0.19261342,\n",
       "       -0.01349959,  0.0618957 ,  0.23838973, -0.39565119, -0.30562469,\n",
       "       -0.14384595,  0.54504538, -0.11173304, -0.32452413, -0.03220934,\n",
       "       -0.55871749,  0.15264036,  0.27756235, -0.23780255,  0.17878136,\n",
       "        0.14989513,  0.24838915, -0.00451216,  0.01123409, -0.0388257 ,\n",
       "       -0.14114259, -0.03960733, -0.07071549,  0.03337344,  0.06639367], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_model.get_word_vector(\"Wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector arithmetic in continuous space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you may have seen visualizations of word vectors and word relationships on several occasions. But does this actually work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.array(f.get_word_vector('wine'))\n",
    "z=np.array(f.get_word_vector('france'))\n",
    "x=x-z\n",
    "\n",
    "y=np.array(f.get_word_vector('beer'))\n",
    "b=np.array(f.get_word_vector('germany'))\n",
    "y=y-b\n",
    "\n",
    "x = x.reshape(1,-1)\n",
    "y = y.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
