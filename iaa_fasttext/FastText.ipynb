{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7d0786a6-6c79-4f19-a389-4921dffdeab4"
    }
   },
   "source": [
    "# FastText - Word Vectors & Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "da2b60e3-d773-4738-98a8-f84f51b32fc5"
    }
   },
   "source": [
    "Hello and welcome to the IAA FastText tutorial!\n",
    "\n",
    "In this tutorial, we will dive into the following topics:\n",
    "\n",
    "- Word vectors and word space arithmetic\n",
    "- Text classification\n",
    "\n",
    "You will be able to experiment freely with word vectors and then move onto classifying text into categories. So, let's get started!\n",
    "\n",
    "We'll be using FastText from Facebook throughout this tutorial. FastText is a highly optimized open-source tool that serves the following three purposes:\n",
    "\n",
    "- Learning vector representations for words. See [this paper](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "- Classifying text into categories. See [this paper](https://arxiv.org/pdf/1607.01759.pdf).\n",
    "- Compressing these models to work on mobile devices. See [this paper](https://arxiv.org/pdf/1612.03651.pdf).\n",
    "\n",
    "To start working on this notebook, please **select the Python 3 kernel** via Kernel > Change kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4739d6be-7fc5-4d53-9a48-c9ad032717d7"
    }
   },
   "source": [
    "## Word representations via Skipgram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1cf13f92-6987-4cdd-ade6-3e0cbe367e2e"
    }
   },
   "source": [
    "In 2013, the research group around Thomas Mikolov at Google [introduced](https://arxiv.org/pdf/1301.3781.pdf) two models for learning vector representations for words from very large data sets. In this tutorial, we will concentrate on the Skipgram model.\n",
    "\n",
    "The skipgram model is surprisingly simple: Given a sentence, it predicts the surrounding words given the current word. Due to limited time in this tutorial, we cannot explain this model in detail. Please refer to [the original paper](https://arxiv.org/pdf/1301.3781.pdf), [our presentation in the team meeting](https://orangesharing.com/confluence/download/attachments/35651640/DOCLA%20FINAL_v3.pptx?version=1&modificationDate=1510305214047&api=v2) as well as [this excellent blog post](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).\n",
    "\n",
    "Facebook provides pre-trained models trained on Wikipedia. We have prepared these models in this workspace. So, let's load them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "94a65678-1567-4e63-b987-ae104b16b5ba"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "from scipy import spatial\n",
    "%pylab inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "0804d2e9-a4e1-422c-8367-eecd4b030edf"
    }
   },
   "outputs": [],
   "source": [
    "vec_model = fastText.load_model(\"/home/iaa/lib/fastText/data/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f95470cc-45bb-46c8-b0de-5857dccd1688"
    }
   },
   "source": [
    "We have now loaded the pre-trained skipgram model. Note that this model was trained using sub-word information, meaning that a word is composed of a set of n-grams. We can see the n-grams a made is up of using the `get_subwords` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "4335c89e-7ab9-420f-806f-2f9dbc773b46"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['wikipedia',\n",
       "  '<wi',\n",
       "  '<wik',\n",
       "  '<wiki',\n",
       "  '<wikip',\n",
       "  'wik',\n",
       "  'wiki',\n",
       "  'wikip',\n",
       "  'wikipe',\n",
       "  'iki',\n",
       "  'ikip',\n",
       "  'ikipe',\n",
       "  'ikiped',\n",
       "  'kip',\n",
       "  'kipe',\n",
       "  'kiped',\n",
       "  'kipedi',\n",
       "  'ipe',\n",
       "  'iped',\n",
       "  'ipedi',\n",
       "  'ipedia',\n",
       "  'ped',\n",
       "  'pedi',\n",
       "  'pedia',\n",
       "  'pedia>',\n",
       "  'edi',\n",
       "  'edia',\n",
       "  'edia>',\n",
       "  'dia',\n",
       "  'dia>',\n",
       "  'ia>'],\n",
       " array([    104, 3464641, 4459358, 3986705, 4499551, 3641052, 2981995,\n",
       "        4003405, 4022108, 3360838, 3088430, 3734365, 2960415, 3583541,\n",
       "        4103636, 3494260, 2885315, 3309957, 3864535, 3689398, 3311169,\n",
       "        3519608, 3537807, 3432822, 4513568, 3119881, 3214276, 3085910,\n",
       "        3969639, 2531043, 2799581]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_model.get_subwords(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "de89e2fe-26f3-477c-b608-4fc084c1e1e2"
    }
   },
   "source": [
    "In the previous output, the `<` and `>` indicate the beginning and end of a word.\n",
    "\n",
    "The vector representation of a word can be queries using the `get_word_vector` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "efb45173-2997-4586-abcf-bcaf527f72aa"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.69488209e-01,  -3.06198329e-01,  -4.78578627e-01,\n",
       "         3.66119631e-02,  -3.71713400e-01,  -1.29155189e-01,\n",
       "         3.08121353e-01,  -6.10210359e-01,  -4.26935256e-01,\n",
       "         2.64354289e-01,   1.28045790e-02,   3.42148066e-01,\n",
       "         6.46890253e-02,   5.30238673e-02,  -2.96356902e-02,\n",
       "        -2.14672059e-01,  -1.80559069e-01,   1.27842560e-01,\n",
       "         5.68398312e-02,   4.88140643e-01,  -2.61829883e-01,\n",
       "         5.56036174e-01,   2.24148571e-01,   2.22534016e-01,\n",
       "        -1.85583994e-01,  -1.10030048e-01,  -1.90432772e-01,\n",
       "        -3.12642530e-02,  -3.27409059e-02,  -1.52658150e-01,\n",
       "        -1.69946402e-01,  -1.20262094e-01,  -3.58753651e-01,\n",
       "         3.11426520e-01,  -4.05167937e-01,  -3.54144722e-01,\n",
       "        -2.18132645e-01,   3.32609594e-01,  -1.36299971e-02,\n",
       "         8.77231508e-02,  -4.32297498e-01,  -3.00237298e-01,\n",
       "        -1.92881003e-01,  -1.04065567e-01,   8.00600201e-02,\n",
       "         5.41281223e-01,  -1.00463189e-01,  -1.50544688e-01,\n",
       "         7.27532152e-03,  -2.45113485e-02,  -5.90430526e-03,\n",
       "        -3.93262625e-01,   7.51490220e-02,  -2.50625890e-03,\n",
       "        -2.14006484e-01,   1.10835657e-01,  -1.34021655e-01,\n",
       "        -2.46824443e-01,   3.73594761e-02,   4.92638171e-01,\n",
       "        -2.46628627e-01,   1.32235819e-02,   4.09391612e-01,\n",
       "        -2.69212276e-01,  -3.12852323e-01,  -2.50444710e-01,\n",
       "         4.86063451e-01,   1.11833058e-01,   5.34872077e-02,\n",
       "         1.42374068e-01,   7.11542606e-01,   2.03008890e-01,\n",
       "        -3.14312190e-01,  -1.24851234e-01,  -6.70968974e-03,\n",
       "         1.05087660e-01,   1.79545403e-01,   3.72583002e-01,\n",
       "         3.70100215e-02,  -3.32040459e-01,  -3.76541056e-02,\n",
       "         8.53010952e-01,   1.19349873e-02,   2.80285358e-01,\n",
       "         3.56130451e-02,   4.45503920e-01,  -2.97871679e-01,\n",
       "         6.34625107e-02,   6.07850365e-02,   2.08806992e-01,\n",
       "         3.93766075e-01,   3.71177904e-02,   2.07051992e-01,\n",
       "        -4.65051949e-01,   4.21654016e-01,   2.19016418e-01,\n",
       "        -5.24018943e-01,  -5.34422636e-01,   1.84835151e-01,\n",
       "         1.93766251e-01,   2.75508016e-01,  -1.38915852e-01,\n",
       "         4.63911176e-01,  -3.22440639e-02,   4.06800121e-01,\n",
       "         2.80161768e-01,  -3.81290942e-01,   7.36551344e-01,\n",
       "         5.69901168e-01,   2.58361995e-01,  -3.37360889e-01,\n",
       "        -1.52926221e-01,   2.59278655e-01,  -3.15787554e-01,\n",
       "        -3.25759947e-01,   3.19289118e-02,  -1.45448223e-01,\n",
       "         3.59561116e-01,   9.83740836e-02,   9.83408689e-02,\n",
       "         2.62850791e-01,   1.30840885e-02,  -4.17474173e-02,\n",
       "         2.79741585e-01,   3.44350368e-01,   1.14444494e-01,\n",
       "        -1.63115472e-01,   5.94461076e-02,  -1.77657604e-03,\n",
       "         4.10003364e-01,  -3.85064222e-02,  -4.89818901e-01,\n",
       "        -2.50878304e-01,   6.31696805e-02,  -1.35764048e-01,\n",
       "         2.91380852e-01,  -3.93040806e-01,  -1.85234267e-02,\n",
       "         3.18686187e-01,   5.59519708e-01,  -4.12121743e-01,\n",
       "         3.79650295e-01,   4.38382536e-01,  -9.86576453e-03,\n",
       "        -1.81347251e-01,  -3.11793864e-01,   2.05835432e-01,\n",
       "         3.10794450e-02,  -2.78870612e-01,  -3.12621266e-01,\n",
       "         5.10007441e-01,  -1.01354286e-01,   2.93388605e-01,\n",
       "        -2.04453301e-02,  -2.69063234e-01,  -3.45784217e-01,\n",
       "        -2.67554279e-02,  -2.42360439e-02,  -1.07837297e-01,\n",
       "         1.58292845e-01,  -4.06798482e-01,   2.68968076e-01,\n",
       "        -5.24762392e-01,  -3.75273228e-01,   2.63094276e-01,\n",
       "        -6.98330164e-01,   3.19911875e-02,  -4.34392363e-01,\n",
       "        -4.13836585e-03,   3.13667804e-01,   1.14905804e-01,\n",
       "        -2.01833993e-01,  -6.63089156e-01,  -3.49152237e-01,\n",
       "        -1.56092405e-01,   1.50233105e-01,   2.49449722e-02,\n",
       "        -8.29046767e-04,   3.76629382e-01,  -2.37408072e-01,\n",
       "        -3.05797666e-01,   2.93393314e-01,  -2.31118768e-01,\n",
       "         3.67172182e-01,  -2.42290050e-01,   1.09208256e-01,\n",
       "         1.72161832e-01,  -1.62638962e-01,   3.99878286e-02,\n",
       "        -3.23145986e-02,  -3.08718741e-01,  -2.64258802e-01,\n",
       "         8.00847784e-02,   1.68589130e-01,   1.12734109e-01,\n",
       "        -3.60000670e-01,  -1.44896165e-01,   3.28692138e-01,\n",
       "         1.57102853e-01,  -1.65414408e-01,  -3.28228921e-02,\n",
       "        -3.12989175e-01,  -1.67593822e-01,  -1.01390898e-01,\n",
       "         1.46922693e-02,   3.63687724e-01,  -8.29925239e-01,\n",
       "        -5.20344675e-02,   2.89718628e-01,  -1.81798279e-01,\n",
       "         3.44743095e-02,   3.04403007e-02,  -9.13990885e-02,\n",
       "        -1.89252749e-01,  -5.11620522e-01,  -2.85670877e-01,\n",
       "        -1.92611560e-01,   8.34961073e-04,   9.46027562e-02,\n",
       "         1.38574854e-01,   1.58768550e-01,   2.54824191e-01,\n",
       "        -8.76645893e-02,  -3.42182182e-02,   1.17350511e-01,\n",
       "         6.39333546e-01,  -6.87678829e-02,  -3.71303082e-01,\n",
       "        -5.01940072e-01,   2.13910013e-01,   5.48887141e-02,\n",
       "         1.18801884e-01,  -6.97920978e-01,  -8.62906948e-02,\n",
       "         2.22803891e-01,  -4.06926513e-01,   2.00612426e-01,\n",
       "         1.49483234e-01,   2.31792197e-01,  -2.10504189e-01,\n",
       "        -1.69266343e-01,   5.17602125e-03,   1.72883779e-01,\n",
       "        -8.49686097e-03,   1.27753057e-02,   3.53843421e-02,\n",
       "         2.32475206e-01,   5.51661670e-01,   1.97488189e-01,\n",
       "         1.48774922e-01,  -1.71288207e-01,   1.49955779e-01,\n",
       "        -4.85951692e-01,   3.43601018e-01,   5.15343547e-01,\n",
       "         2.50053406e-03,   8.05562139e-02,  -3.18373322e-01,\n",
       "         5.42744219e-01,   4.70065288e-02,   2.91820467e-01,\n",
       "        -1.21324487e-01,  -2.43086461e-02,   1.16893880e-01,\n",
       "        -2.05527782e-01,   2.35454328e-02,  -2.42745087e-01,\n",
       "         2.97870487e-01,  -2.30436370e-01,  -5.15177883e-02,\n",
       "         2.82517076e-01,  -5.09017892e-02,   5.38300872e-01,\n",
       "         2.46370152e-01,   1.97216007e-03,   4.75761481e-02,\n",
       "         4.02466893e-01,   2.24489853e-01,  -4.94233161e-01,\n",
       "        -3.54628623e-01,  -7.48653933e-02,   6.44630432e-01,\n",
       "        -4.90400754e-02,   7.09454045e-02,  -4.62708212e-02,\n",
       "        -7.01132596e-01,   6.88813850e-02,   5.15985966e-01,\n",
       "        -4.42822874e-01,  -1.95453353e-02,   3.82521719e-01,\n",
       "         2.09696740e-01,   8.27086344e-03,  -2.89491475e-01,\n",
       "         7.09176660e-02,  -1.47160470e-01,  -1.37597084e-01,\n",
       "         3.07852060e-01,   6.09674118e-02,   1.13823721e-02], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_model.get_word_vector(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained with the tuning parameter dim set to 300. Hence, a word vector is just a vector of 300 numbers in continuous space.\n",
    "\n",
    "**<span style=\"color:red\">Please note that you should lowercase all words because the model has been trained with lowercase input!</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "268f86dc-4008-473f-b8e7-2896c24ab5e2"
    }
   },
   "source": [
    "## Word vector arithmetic in continuous space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dbf4c289-93d9-47d0-b5b3-3ceabdab392d"
    }
   },
   "source": [
    "By now, you may have seen visualizations of word vectors and word relationships on several occasions. But does this actually work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = vec_model.get_word_vector(\"wine\")\n",
    "beer = vec_model.get_word_vector(\"beer\")\n",
    "france = vec_model.get_word_vector(\"france\")\n",
    "germany = vec_model.get_word_vector(\"germany\")\n",
    "cognac = vec_model.get_word_vector(\"cognac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61361015]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity((wine - france).reshape(1, -1), (beer - germany).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37843513]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity((wine - france).reshape(1, -1), (cognac - germany).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similary for (beer - Germany) is much higher than (wine - Germany)!\n",
    "\n",
    "We can also do more arithmetic in conjunction with a tree search to search for the solution of an equation.\n",
    "\n",
    "For example, let's try to find what country would be equivalent to German without beer but with wine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "word_list = vec_model.get_words()\n",
    "\n",
    "for i, word in enumerate(word_list[:100000]):\n",
    "    vec = vec_model.get_word_vector(word)\n",
    "    A.append(vec)\n",
    "    word_list.append(word)\n",
    "    \n",
    "T = spatial.KDTree(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(formula, k=5):\n",
    "    dists, indices = T.query(formula, k=k)\n",
    "    words = [word_list[i] for i in indices]\n",
    "    \n",
    "    return dists, indices, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = vec_model.get_word_vector(\"germany\") - vec_model.get_word_vector(\"beer\") + vec_model.get_word_vector(\"wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.06677678,  4.72205323,  4.82766514,  5.14881981,  5.19047626]),\n",
       " array([ 582,  947,  479, 2306, 1128]),\n",
       " ['germany', 'italy', 'france', 'switzerland', 'spain'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest(find)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second match already corresponds to Italy and France! Feel free to try something else!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are using fastText to train a classification model on the AG News corpus to categorize news article into the following four classes:\n",
    "\n",
    "- World (index 1)\n",
    "- Sports (index 2)\n",
    "- Business (index 3)\n",
    "- Science & Tech (index 4)\n",
    "\n",
    "The corpus is comprised of one million news articles. Note that for performance reasons and the limited time during the workshop, we are not using pre-trained word vectors in this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__3 , bankruptcy plan #39 will not deter #39 yukos sale , yukos shareholders gave warning yesterday that the oil giants attempt to seek bankruptcy protection in the us was unlikely to prevent the russian government-ordered \r\n",
      "__label__2 , california beats n . carolina 9-2 in llws ( ap ) , ap - danny leon hit a two-run homer , and tyler carp and john lister added solo homers to lead conejo valley little league to a 9-2 victory over morganton , n . c . , on sunday in the little league world series . \r\n",
      "__label__3 , stocks open lower as oil nears \\$50/barrel , us stocks opened lower on monday as investor concerns were fueled by oil prices hitting a fresh record high , marching closer to \\$50 a barrel , on supply concerns emerging from nigeria . \r\n"
     ]
    }
   ],
   "source": [
    "!head -n3 ~/lib/fastText/data/ag_news.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the data set has already been pre-processed to a data format suitable for fastText. For supervised classification models, fastText provides a `train_supervised` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_supervised in module fastText.FastText:\n",
      "\n",
      "train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='', saveOutput=0)\n",
      "    Train a supervised model and return a model object.\n",
      "    \n",
      "    input must be a filepath. The input text does not need to be tokenized\n",
      "    as per the tokenize function, but it must be preprocessed and encoded\n",
      "    as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "    as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "    \n",
      "    The input file must must contain at least one label per line. For an\n",
      "    example consult the example datasets which are part of the fastText\n",
      "    repository such as the dataset pulled by classification-example.sh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.train_supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's arguments are as follows:\n",
    "\n",
    "- `lr`: the learning rate of the neural network\n",
    "- `dim`: the dimension of the embedding vectors to be trained; if using pre-trained embeddings, the dimension should match the dimension of these vectors\n",
    "- `ws`: size of the context window\n",
    "- `epoch: the number of times to go over the training data\n",
    "- `minCount`: minimal number of word occurences\n",
    "- `wordNgrams`: the maximum n in n-grams to compute\n",
    "- `buckets`: number of buckets for feature hashing\n",
    "- `minn`: minimum length of char ngrams (for subword embeddings)\n",
    "- `maxn`: maximum length of char ngrams (for subword embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our first model! Note that the Python API does not yet fully support everything fastText provides and only supports file-based data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fastText.train_supervised(\"/home/iaa/lib/fastText/data/ag_news.train\", lr=0.5, minn=1, maxn=1, wordNgrams=5, minCount=1, bucket=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t7600\r\n",
      "P@1\t0.915\r\n",
      "R@1\t0.915\r\n",
      "Number of examples: 7600\r\n"
     ]
    }
   ],
   "source": [
    "!~/lib/fastText/fasttext test ~/proj/ing_tutorials/iaa_fasttext/model ~/lib/fastText/data/ag_news.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It already works quite okay. However, when further inspecting the model, we find the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([ 1.]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"nuclear test in north korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__3',), array([ 0.81640625]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"nuclear?? test in north korea??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the model to use sub-word embeddings by setting the maxn parameter to > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "model2 = fastText.train_supervised(\"/home/iaa/lib/fastText/data/ag_news.train\", lr=0.5, minn=1, maxn=?, wordNgrams=5, minCount=1, bucket=10000000)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the predictions for `nuclear test in north korea` and `nuclear?? test in north korea??`. Can you explain the difference to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([ 1.]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(\"nuclear test in north korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([ 0.99804687]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(\"nuclear?? test in north korea??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment freely with fastText. What is the best performance you can achieve? If you want, you can also compare it to other approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
